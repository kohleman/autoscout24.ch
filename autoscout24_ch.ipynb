{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoscout24.ch Price Monitor\n",
    "\n",
    "The idea is to get an automated (eventually daily) price overview for a specific type of car. But also exploratory data analysis done to find some patterns or the 'best' valued car for money. \n",
    "As Autoscout24.ch does not offer a free API, we use web scraping to get the data we are interested in. \n",
    "Weak point in this approach is the possibility of changes in the HTML structure. This would require changes also in the Jupyter notebook. But this lies in the nature of web scraping.\n",
    "\n",
    "(c) Manuel Kohler, Basel, Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/kohleman/Library/Python/3.7/lib/python/site-packages (1.15.1)\n",
      "3.7.0 (v3.7.0:1bf9cc5093, Jun 26 2018, 23:26:24) \n",
      "[Clang 6.0 (clang-600.0.57)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "!{sys.executable} -m pip install numpy\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page:  1\n",
      "Loading page:  2\n",
      "Loading page:  3\n",
      "Loading page:  4\n",
      "Loading page:  5\n",
      "Loading page:  6\n",
      "No more cars found after 5 iterations\n",
      "Found 88 cars\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "baseurl = 'https://www.autoscout24.ch'\n",
    "bmwi3_baseurl = baseurl + '/de/autos/bmw--i3'\n",
    "# default is 20 cars per page, which would mean 20*10 cars\n",
    "maxpages = 10\n",
    "\n",
    "link_list = [] \n",
    "\n",
    "for i in range(1, maxpages):\n",
    "    # fuel:16 = electro only, no REX\n",
    "    # make:9 = BMW\n",
    "    # model:1949 = i3    \n",
    "    payload = {'fuel':16, 'make': '9', 'model': '1949', 'page': i, 'st': 1, 'vehtyp': 10, 'sort': 'price_asc'}\n",
    "    \n",
    "    # Tesla S\n",
    "#     payload = {'fuel':16, 'make': '391', 'model': '1919', 'page': i, 'st': 1, 'vehtyp': 10, 'sort': 'price_asc'}\n",
    "\n",
    "    # VW e-Golf\n",
    "#     payload = {'fuel':16, 'make': '82', 'model': '585', 'page': i, 'st': 1, 'vehtyp': 10, 'sort': 'price_asc'}\n",
    "\n",
    "    # VW Petrol Golf\n",
    "#     payload = {'fuel':14, 'make': '82', 'model': '585', 'page': i, 'st': 1, 'vehtyp': 10, 'sort': 'price_asc'}\n",
    "\n",
    "\n",
    "    print('Loading page: ', i)    \n",
    "    r = requests.get(bmwi3_baseurl, params=payload)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "#     print(soup.prettify(formatter=None))\n",
    "    car_links = soup.find_all(\"a\", class_=\"primary-link\")\n",
    "\n",
    "    # if no more new cars found we can exit the for loop. \n",
    "    # maxpages is then bigger than the real number of cars\n",
    "    if (len(car_links) == 0):\n",
    "        print(f\"No more cars found after {i-1} iterations\")\n",
    "        break\n",
    "    for car_link in car_links:\n",
    "        link_list.append(car_link)\n",
    "        \n",
    "print('Found ' + str(len(link_list)) + ' cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 10 cars\n",
      "Downloaded 20 cars\n",
      "Downloaded 30 cars\n",
      "Downloaded 40 cars\n",
      "Downloaded 50 cars\n",
      "Downloaded 60 cars\n",
      "Downloaded 70 cars\n",
      "Downloaded 80 cars\n",
      "Downloaded 88 cars details in total.\n"
     ]
    }
   ],
   "source": [
    "# data = {}\n",
    "car_list = []\n",
    "\n",
    "data_fields = [\"Inverkehrsetzung\", \"Fahrzeugart\", \"Aussenfarbe\", \"Kilometer\", \"Getriebeart\", \n",
    "               \"Antriebsart\", \"Treibstoff\", \"TÃ¼ren\", \"Sitze\", \"Innenfarbe\", \"Hubraum\",\n",
    "               \"Zylinder\", \"PS\", \"Leergewicht\", \"Verbrauch in l/100 km\",\"CO2-Emission\",\n",
    "               \"Energieeffizienz\", \"Euro Norm\", \"Wagen-Nr.\", \"Ab MFK \", \"Direkt-/Parallelimport\",\n",
    "               \"Garantie\", \"Neupreis\", \"Preis\"]\n",
    "\n",
    "for carIdx, carVal in enumerate(link_list):\n",
    "    car = {}\n",
    "#     value_list = []\n",
    "#     property_list = []\n",
    "\n",
    "#     print(carVal)\n",
    "        \n",
    "    if (carIdx % 10 == 0) and (carIdx > 0):\n",
    "        print(f\"Downloaded {carIdx} cars\")\n",
    "    individual_car_request = requests.get(baseurl + carVal.get('href'))\n",
    "    soup = BeautifulSoup(individual_car_request.text, 'html.parser')\n",
    "    \n",
    "    # extract the individual vehical id given by autoscout, we assume that those are unique\n",
    "    vehid = carVal.get('href').split(\"&\")[7].split(\"=\")[1]\n",
    "    title = carVal.get('title')\n",
    "    \n",
    "    car['vehid'] = vehid\n",
    "    car['carLink'] = baseurl + '/' + vehid\n",
    "    car['title'] = title\n",
    "    \n",
    "    car_textlist_item = soup.find_all(\"li\", class_=\"textlist-item\")\n",
    "\n",
    "    prop = soup.find_all(\"div\", class_=\"prop\")\n",
    "    value = soup.find_all(\"div\", class_=\"value\")\n",
    "      \n",
    "    # I assume that a property has always a matching value!\n",
    "    for idx, val in enumerate(prop):\n",
    "#         print(val.get_text().strip())\n",
    "        car[val.get_text().strip()] = value[idx].get_text().strip()\n",
    "#         property_list.append(val.get_text().strip())\n",
    "#         value_list.append(value[idx].get_text().strip())\n",
    "#         data[vehid] = value_list[0:17]\n",
    "        \n",
    "    car_list.append(car)\n",
    "\n",
    "print(f\"Downloaded {carIdx+1} cars details in total.\")\n",
    "#     print(len(value_list))\n",
    "\n",
    "# print(car_list)\n",
    "\n",
    "car_df = pd.DataFrame(car_list, dtype=object).fillna('0')\n",
    "\n",
    "# print(property_list)\n",
    "# print(data)\n",
    "#     print(car['Preis'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 400)\n",
    "# pd.set_option('display.max_colwidth', 10)\n",
    "\n",
    "\n",
    "# interesting_df = car_df[['vehid', 'title', 'Inverkehrsetzung','Kilometer', 'Preis', 'Neupreis', 'carLink']]\n",
    "interesting_df = car_df[['vehid', 'title', 'Inverkehrsetzung','Kilometer', 'Preis', 'carLink']]\n",
    "\n",
    "\n",
    "abc = interesting_df.copy()\n",
    "\n",
    "# Thanks for this hint:\n",
    "# https://stackoverflow.com/questions/42263946/how-to-create-a-table-with-clickable-hyperlink-in-pandas-jupyter-notebook\n",
    "def make_clickable(val):\n",
    "    # target _blank to open new window\n",
    "    return '<a target=\"_blank\" href=\"{}\">Link</a>'.format(val)\n",
    "\n",
    "interesting_df.style.format({'carLink': make_clickable})\n",
    "\n",
    "interesting_df.Preis\n",
    "\n",
    "\n",
    "def only_numbers(Preis):\n",
    "    latestPreis = Preis.split('\\n')[0]\n",
    "    p = re.compile('[^\\d]+')\n",
    "    return int(p.sub('', latestPreis))\n",
    "\n",
    "\n",
    "abc['cleanPrice'] = abc.Preis.apply(only_numbers)\n",
    "abc['cleanKilometer'] = abc.Kilometer.apply(only_numbers)\n",
    "\n",
    "new_df = abc[['vehid', 'title', 'cleanKilometer', 'cleanPrice']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Missing on the webpage, sort by the latest uploaded cars\n",
    "# I assume that the vehid is similar to a DB sequence\n",
    "# interesting_df.sort_values(by=['vehid'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "interesting_df.to_csv(f\"./cars_{payload['make']}_{payload['model']}_{now}.csv\", encoding='utf-8')\n",
    "interesting_df.to_excel(f\"./cars_{payload['make']}_{payload['model']}_{now}.xlsx\", sheet_name='Autoscout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is not helpful currently\n",
    "interesting_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# new_df.plot(kind='scatter',x='cleanPrice',y='cleanKilometer',color='red')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('cleanKilometer', 'cleanPrice', data=new_df, fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Let's try a naive way of finding the car with the best value (obvislously a soft target)\n",
    "# which in this simple approach means all the dots which have a short distance to the origin. No features\n",
    "# are considered in this case.\n",
    "# We can easily calculate this using good old Pythagoras\n",
    "\n",
    "a = new_df.copy()\n",
    "a['dist_to_origin'] = np.sqrt((new_df['cleanKilometer'])^2 + (new_df['cleanPrice'])^2)\n",
    "a.sort_values(by=['dist_to_origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
